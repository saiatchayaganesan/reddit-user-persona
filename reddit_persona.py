# -*- coding: utf-8 -*-
"""reddit_persona.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eNZ4HwQ3fxYXhNhquvRz3815ESIGO2i1
"""

!pip install -q praw nltk

import praw
import re
import collections
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

# 🔐 Reddit API credentials
REDDIT_CLIENT_ID = "wSjTchR5b9XRymVBRHEqyg"
REDDIT_CLIENT_SECRET = "9lpcCpSu_xNfoeQ98p6IimiHGXslqA"
USER_AGENT = "RedditPersonaApp by saiatchaya"

def setup_reddit_api():
    return praw.Reddit(client_id=REDDIT_CLIENT_ID,
                       client_secret=REDDIT_CLIENT_SECRET,
                       user_agent=USER_AGENT)

def get_reddit_data(username):
    reddit = setup_reddit_api()
    user = reddit.redditor(username)
    posts, comments = [], []

    try:
        for post in user.submissions.new(limit=10):
            posts.append(post.title + " " + post.selftext)
        for comment in user.comments.new(limit=10):
            comments.append(comment.body)
    except Exception as e:
        print(f"❌ Error fetching data: {e}")

    return posts, comments

def identify_topics(posts, comments):
    text = ' '.join(posts + comments)
    words = [word.lower() for word in text.split() if word.isalpha()]

    stop_words = set(stopwords.words('english'))
    custom_ignores = set(["one", "two", "three", "multiple", "many", "time", "thing", "new", "old", "get", "got", "make", "made", "day", "year", "month", "person", "people"])

    filtered_words = [word for word in words if word not in stop_words and word not in custom_ignores and len(word) > 3]

    freq = collections.Counter(filtered_words)
    return [word for word, count in freq.most_common(5)]

def analyze_tone(texts):
    score = 0
    for text in texts:
        score += len(re.findall(r'\b(happy|great|love|awesome|enjoy|fun|cool)\b', text.lower()))
        score -= len(re.findall(r'\b(sad|hate|bad|angry|worst|annoyed|boring)\b', text.lower()))
    if score > 2:
        return "positive"
    elif score < -2:
        return "negative"
    else:
        return "neutral"

def estimate_persona(username, topics, tone, post_count, comment_count):
    activity = "an active Redditor" if post_count + comment_count > 10 else "a casual Reddit user"
    return (
        f"{username} is {activity} who frequently engages with topics like {', '.join(topics)}. "
        f"Their overall language tone seems to be {tone}. "
        f"They appear to communicate openly through both posts and comments."
    )

def save_persona_to_txt(username, persona, posts, comments, topics, tone):
    file_name = f"{username}_persona.txt"
    with open(file_name, 'w') as f:
        f.write("🔍 Reddit User Persona\n")
        f.write(f"👤 Username: {username}\n\n")
        f.write("📌 Inferred Traits:\n")
        f.write(f"- Topics of Interest: {', '.join(topics)}\n")
        f.write(f"- Language Tone: {tone}\n")
        f.write(f"- Number of Posts: {len(posts)}\n")
        f.write(f"- Number of Comments: {len(comments)}\n\n")
        f.write("🧠 Persona Summary:\n")
        f.write(persona + "\n\n")
        f.write("📄 Citations:\n\n")
        for i, post in enumerate(posts):
            f.write(f"[Post {i+1}] {post[:300]}...\nUsed for: Topics & Tone\n\n")
        for i, comment in enumerate(comments):
            f.write(f"[Comment {i+1}] {comment[:300]}...\nUsed for: Topics & Tone\n\n")
    print(f"✅ Persona saved to: {file_name}")

def build_user_persona():
    username = input("Enter Reddit username: ").strip()
    posts, comments = get_reddit_data(username)

    if not posts and not comments:
        print("⚠️ No content found for this user.")
        return

    topics = identify_topics(posts, comments)
    tone = analyze_tone(posts + comments)
    persona_summary = estimate_persona(username, topics, tone, len(posts), len(comments))

    print("\n🧠 Persona Summary:\n")
    print(persona_summary)

    save_persona_to_txt(username, persona_summary, posts, comments, topics, tone)

# 🚀 Call the function
build_user_persona()